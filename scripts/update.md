1. 指定可能な活性化関数に 'relu' を追加

2. Stacked の際に fe,fd の引数をリストで渡すことで層ごとに活性化関数を設定できるように変更。
当然ながら、階層数とリストの要素数を合わせる必要がある。また、文字列を渡した場合はその活性化関数が全層に適用される。
また、元の仕様だとエンコーダとデコーダの活性化関数を別のものにすると問題点があったと思われる。具体例として活性化関数を、fe='relu',fd='sigmoid' にした場合を考える。第1層のエンコーダはreluによって0以上の値に、第2層でのデコーダはsigmoidによって0~1に変更される。このとき第2層の入力に1以上の値が含まれていた場合、元の波形の再構成は不可能となる。

3. オートエンコーダは最後に全層通して学習し直すものだとどっかで聞いたのでtrain\_all関数を追加。一応重みが更新されていることは確認しました。


